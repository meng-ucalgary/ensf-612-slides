{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "conda_python3",
      "language": "python",
      "name": "conda_python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Pyspark: NLP Example.ipynb",
      "provenance": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBb2TDY0H5MY"
      },
      "source": [
        "### Initialize the cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dERJyU0BEtMy",
        "outputId": "981494e4-2a5b-4333-b0ea-e4dc47f4acde"
      },
      "source": [
        "import os\n",
        "import atexit\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import pyspark\n",
        "from pyspark.context import SparkContext\n",
        "from pyspark.sql import SQLContext\n",
        "import findspark\n",
        "from sparkhpc import sparkjob\n",
        "\n",
        "#Exit handler to clean up the Spark cluster if the script exits or crashes\n",
        "def exitHandler(sj,sc):\n",
        "    try:\n",
        "        print('Trapped Exit cleaning up Spark Context')\n",
        "        sc.stop()\n",
        "    except:\n",
        "        pass\n",
        "    try:\n",
        "        print('Trapped Exit cleaning up Spark Job')\n",
        "        sj.stop()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "#Parameters for the Spark cluster\n",
        "nodes=1\n",
        "tasks_per_node=4 \n",
        "memory_per_task=4096 #4 gig per process, adjust accordingly\n",
        "# Please estimate walltime carefully to keep unused Spark clusters from sitting \n",
        "# idle so that others may use the resources.\n",
        "walltime=\"1:00\" #1 hours\n",
        "#os.environ['SBATCH_PARTITION']='cpu2019' #Set the appropriate ARC partition\n",
        "\n",
        "sj = sparkjob.sparkjob(\n",
        "     ncores=nodes*tasks_per_node,\n",
        "     cores_per_executor=tasks_per_node,\n",
        "     memory_per_core=memory_per_task,\n",
        "     walltime=walltime\n",
        "    )\n",
        "\n",
        "sj.wait_to_start()\n",
        "time.sleep(60)\n",
        "sc = sj.start_spark()\n",
        "\n",
        "#Register the exit handler                                                                                                     \n",
        "atexit.register(exitHandler,sj,sc)\n",
        "\n",
        "#You need this line if you want to use SparkSQL\n",
        "spark=SQLContext(sc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:sparkhpc.sparkjob:Submitted batch job 2121\n",
            "\n",
            "INFO:sparkhpc.sparkjob:Submitted cluster 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ml5rB7JCH-1n"
      },
      "source": [
        "## NLP Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CJBwK-V6M-Yc"
      },
      "source": [
        "### Pyspark"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjOhZC4fEgP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a810b04f-299a-4527-c20b-9e4d56bf217e"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# spark = SparkSession.builder.appName('nlp').getOrCreate()\n",
        "df = spark.read.csv('SMSSpamCollection', inferSchema = True, sep = '\\t')\n",
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mgQzc-9zEgP-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e05101b4-8754-43d6-934a-9753362e6bfb"
      },
      "source": [
        "df = df.withColumnRenamed('_c0', 'class').withColumnRenamed('_c1', 'text')\n",
        "df.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|class|                text|\n",
            "+-----+--------------------+\n",
            "|  ham|Go until jurong p...|\n",
            "|  ham|Ok lar... Joking ...|\n",
            "| spam|Free entry in 2 a...|\n",
            "|  ham|U dun say so earl...|\n",
            "|  ham|Nah I don't think...|\n",
            "| spam|FreeMsg Hey there...|\n",
            "|  ham|Even my brother i...|\n",
            "|  ham|As per your reque...|\n",
            "| spam|WINNER!! As a val...|\n",
            "| spam|Had your mobile 1...|\n",
            "|  ham|I'm gonna be home...|\n",
            "| spam|SIX chances to wi...|\n",
            "| spam|URGENT! You have ...|\n",
            "|  ham|I've been searchi...|\n",
            "|  ham|I HAVE A DATE ON ...|\n",
            "| spam|XXXMobileMovieClu...|\n",
            "|  ham|Oh k...i'm watchi...|\n",
            "|  ham|Eh u remember how...|\n",
            "|  ham|Fine if thatÂ’s th...|\n",
            "| spam|England v Macedon...|\n",
            "+-----+--------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KLMdPVydEgP_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "33c44af2-f860-408d-8be5-45accf6973fe"
      },
      "source": [
        "from pyspark.sql.functions import length\n",
        "\n",
        "df = df.withColumn('length', length(df['text']))\n",
        "df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+------+\n",
            "|class|                text|length|\n",
            "+-----+--------------------+------+\n",
            "|  ham|Go until jurong p...|   111|\n",
            "|  ham|Ok lar... Joking ...|    29|\n",
            "| spam|Free entry in 2 a...|   155|\n",
            "+-----+--------------------+------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7Qt3UTVEgQA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42ef8279-f28a-48ae-8948-6a7e0e37038f"
      },
      "source": [
        "df.groupBy('class').mean().show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+-----------------+\n",
            "|class|      avg(length)|\n",
            "+-----+-----------------+\n",
            "|  ham|71.45431945307645|\n",
            "| spam|138.6706827309237|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b5m-PxiUEgQB"
      },
      "source": [
        "from pyspark.ml.feature import (CountVectorizer, Tokenizer, \n",
        "                                StopWordsRemover, IDF, StringIndexer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LKOYylSEgQB"
      },
      "source": [
        "ham_spam_to_numeric = StringIndexer(inputCol = 'class', outputCol = 'label')\n",
        "tokenizer = Tokenizer(inputCol = 'text', outputCol = 'token_text')\n",
        "stop_remove = StopWordsRemover(inputCol = 'token_text', outputCol = 'stop_token')\n",
        "count_vec = CountVectorizer(inputCol = 'stop_token', outputCol = 'c_vec')\n",
        "idf = IDF(inputCol = 'c_vec', outputCol = 'tf_idf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdnqDQjMEgQC"
      },
      "source": [
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "clean_up = VectorAssembler(inputCols = ['tf_idf', 'length'], outputCol = 'features')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "35Vd7O64EgQD"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes\n",
        "\n",
        "nb = NaiveBayes()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhVSyb5FEgQE"
      },
      "source": [
        "from pyspark.ml import Pipeline\n",
        "\n",
        "pipeline = Pipeline(stages=[ham_spam_to_numeric, tokenizer, stop_remove, count_vec, idf, clean_up])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bu5ht4s0EgQG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73ca7c6b-0600-4b78-a2c4-41f4079a77f3"
      },
      "source": [
        "cleaner = pipeline.fit(df)\n",
        "clean_df = cleaner.transform(df)\n",
        "clean_df = clean_df.select('label', 'features')\n",
        "clean_df.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+\n",
            "|label|            features|\n",
            "+-----+--------------------+\n",
            "|  0.0|(13424,[7,11,31,6...|\n",
            "|  0.0|(13424,[0,24,297,...|\n",
            "|  1.0|(13424,[2,13,19,3...|\n",
            "+-----+--------------------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UYgP4UWpEgQG"
      },
      "source": [
        "train, test = clean_df.randomSplit([0.7, 0.3])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rz9TWMI8EgQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "607ea5a0-9502-416c-fd41-3ec2b5965da9"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- class: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- length: integer (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQcvradOEgQH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aee4b9f-6625-4422-d785-ef2d5c5289b1"
      },
      "source": [
        "spam_detector = nb.fit(train)\n",
        "predictions = spam_detector.transform(test)\n",
        "predictions.show(3)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|label|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "|  0.0|(13424,[0,1,2,13,...|[-626.25690053866...|[0.99999999998055...|       0.0|\n",
            "|  0.0|(13424,[0,1,4,50,...|[-839.21099153942...|[1.0,3.6279256263...|       0.0|\n",
            "|  0.0|(13424,[0,1,5,20,...|[-805.65857603332...|[1.0,7.3924947179...|       0.0|\n",
            "+-----+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 3 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9IYYkPDEgQI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f70a23c7-05a7-4acf-b567-522888ca2fc5"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "\n",
        "evaluator = MulticlassClassificationEvaluator()\n",
        "print(\"Test Accuracy: \" + str(evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy: 0.9271961492178099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vb7JyrGVNDKN"
      },
      "source": [
        "### NLTK + Sklearn"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20TaEDguOU2B"
      },
      "source": [
        "import warnings\n",
        "import numpy as np \n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline    \n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "s9bTi4rWMTHx",
        "outputId": "e9a1e67f-685a-4406-cb00-5128f088e153"
      },
      "source": [
        "data = pd.read_csv('~/projects/spark/SMSSpamCollection', sep='\\t', names=['class', 'text'])\n",
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  class                                               text\n",
              "0   ham  Go until jurong point, crazy.. Available only ...\n",
              "1   ham                      Ok lar... Joking wif u oni...\n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
              "3   ham  U dun say so early hor... U c already then say...\n",
              "4   ham  Nah I don't think he goes to usf, he lives aro..."
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qU8wo0UlNNoo"
      },
      "source": [
        "def Clean(Text):\n",
        "    sms = re.sub('[^a-zA-Z]', ' ', Text) #Replacing all non-alphabetic characters with a space\n",
        "    sms = sms.lower() #converting to lowecase\n",
        "    sms = sms.split()\n",
        "    sms = ' '.join(sms)\n",
        "    return sms\n",
        "\n",
        "data[\"Clean_Text\"] = data[\"text\"].apply(Clean)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-CsHdAxONOE"
      },
      "source": [
        "data[\"Tokenize_Text\"]= data.apply(lambda row: nltk.word_tokenize(row[\"Clean_Text\"]), axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFjcAYVyOqm9"
      },
      "source": [
        "def remove_stopwords(text):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    filtered_text = [word for word in text if word not in stop_words]\n",
        "    return filtered_text\n",
        "\n",
        "data[\"Nostopword_Text\"] = data[\"Tokenize_Text\"].apply(remove_stopwords)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w9X1vjkpPNt0",
        "outputId": "60c65e7d-89cf-4c75-cd35-0c8af985993c"
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /home/ajoy.das/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41N7xtEqOrK5"
      },
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "# lemmatize string\n",
        "def lemmatize_word(text):\n",
        "    # word_tokens = word_tokenize(text)\n",
        "    # provide context i.e. part-of-speech\n",
        "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in text]\n",
        "    return lemmas\n",
        "\n",
        "data[\"Lemmatized_Text\"] = data[\"Nostopword_Text\"].apply(lemmatize_word)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "0BkaaIB4jTIn",
        "outputId": "343b5f88-fe4d-4919-bead-430cb01a551f"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  class                                               text  \\\n",
              "0   ham  Go until jurong point, crazy.. Available only ...   \n",
              "1   ham                      Ok lar... Joking wif u oni...   \n",
              "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...   \n",
              "3   ham  U dun say so early hor... U c already then say...   \n",
              "4   ham  Nah I don't think he goes to usf, he lives aro...   \n",
              "\n",
              "                                          Clean_Text  \\\n",
              "0  go until jurong point crazy available only in ...   \n",
              "1                            ok lar joking wif u oni   \n",
              "2  free entry in a wkly comp to win fa cup final ...   \n",
              "3        u dun say so early hor u c already then say   \n",
              "4  nah i don t think he goes to usf he lives arou...   \n",
              "\n",
              "                                       Tokenize_Text  \\\n",
              "0  [go, until, jurong, point, crazy, available, o...   \n",
              "1                     [ok, lar, joking, wif, u, oni]   \n",
              "2  [free, entry, in, a, wkly, comp, to, win, fa, ...   \n",
              "3  [u, dun, say, so, early, hor, u, c, already, t...   \n",
              "4  [nah, i, don, t, think, he, goes, to, usf, he,...   \n",
              "\n",
              "                                     Nostopword_Text  \\\n",
              "0  [go, jurong, point, crazy, available, bugis, n...   \n",
              "1                     [ok, lar, joking, wif, u, oni]   \n",
              "2  [free, entry, wkly, comp, win, fa, cup, final,...   \n",
              "3      [u, dun, say, early, hor, u, c, already, say]   \n",
              "4     [nah, think, goes, usf, lives, around, though]   \n",
              "\n",
              "                                     Lemmatized_Text  \n",
              "0  [go, jurong, point, crazy, available, bugis, n...  \n",
              "1                       [ok, lar, joke, wif, u, oni]  \n",
              "2  [free, entry, wkly, comp, win, fa, cup, final,...  \n",
              "3      [u, dun, say, early, hor, u, c, already, say]  \n",
              "4        [nah, think, go, usf, live, around, though]  "
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>class</th>\n",
              "      <th>text</th>\n",
              "      <th>Clean_Text</th>\n",
              "      <th>Tokenize_Text</th>\n",
              "      <th>Nostopword_Text</th>\n",
              "      <th>Lemmatized_Text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ham</td>\n",
              "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
              "      <td>go until jurong point crazy available only in ...</td>\n",
              "      <td>[go, until, jurong, point, crazy, available, o...</td>\n",
              "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
              "      <td>[go, jurong, point, crazy, available, bugis, n...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ham</td>\n",
              "      <td>Ok lar... Joking wif u oni...</td>\n",
              "      <td>ok lar joking wif u oni</td>\n",
              "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
              "      <td>[ok, lar, joking, wif, u, oni]</td>\n",
              "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>spam</td>\n",
              "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
              "      <td>free entry in a wkly comp to win fa cup final ...</td>\n",
              "      <td>[free, entry, in, a, wkly, comp, to, win, fa, ...</td>\n",
              "      <td>[free, entry, wkly, comp, win, fa, cup, final,...</td>\n",
              "      <td>[free, entry, wkly, comp, win, fa, cup, final,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ham</td>\n",
              "      <td>U dun say so early hor... U c already then say...</td>\n",
              "      <td>u dun say so early hor u c already then say</td>\n",
              "      <td>[u, dun, say, so, early, hor, u, c, already, t...</td>\n",
              "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
              "      <td>[u, dun, say, early, hor, u, c, already, say]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ham</td>\n",
              "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
              "      <td>nah i don t think he goes to usf he lives arou...</td>\n",
              "      <td>[nah, i, don, t, think, he, goes, to, usf, he,...</td>\n",
              "      <td>[nah, think, goes, usf, lives, around, though]</td>\n",
              "      <td>[nah, think, go, usf, live, around, though]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k59FzN3jO0pq"
      },
      "source": [
        "corpus= []\n",
        "for i in data[\"Lemmatized_Text\"]:\n",
        "    msg = ' '.join([row for row in i])\n",
        "    corpus.append(msg)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTvmefynO1C4",
        "outputId": "83f49d9a-f5f3-4937-a205-c1be3e2ff580"
      },
      "source": [
        "# Changing text data in to numbers. \n",
        "tfidf = TfidfVectorizer()\n",
        "X = tfidf.fit_transform(corpus).toarray()\n",
        "# Let's have a look at our feature \n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5572, 6653)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NCUntN5EO7iR"
      },
      "source": [
        "y = data[\"class\"] \n",
        "# Splitting the testing and training sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LMI2cp8O8BR",
        "outputId": "3aaf8015-c388-4ffc-ee26-e22703eed489"
      },
      "source": [
        "# Testing on the following classifiers\n",
        "classifier = MultinomialNB()\n",
        "# classifier = RandomForestClassifier(),\n",
        "classifier.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSWMCnyOO_Zg",
        "outputId": "8c27c0d7-e2cc-4cdc-9bdf-4477f37e4274"
      },
      "source": [
        "# Cossvalidation \n",
        "cv_score = cross_val_score(classifier, X_train,y_train,scoring=\"accuracy\", cv=10)\n",
        "print(cv_score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9607736324232816\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-txdnyVgp04",
        "outputId": "6e708427-b4cf-4695-d4bc-f291a490d9e4"
      },
      "source": [
        "pred_train = classifier.predict(X_train)\n",
        "pred_test = classifier.predict(X_test)\n",
        "\n",
        "print(f'Train Accuracy: {classifier.score(X_train,y_train)}')\n",
        "print(f'Test Accuracy: {classifier.score(X_test,y_test)}')\n",
        "print(f'Test F1 Score: {metrics.f1_score(y_test, pred_test, average=\"binary\", pos_label=\"ham\")}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Accuracy: 0.9717948717948718\n",
            "Test Accuracy: 0.9683014354066986\n",
            "Test F1 Score: 0.9820642978003383\n"
          ]
        }
      ]
    }
  ]
}